---
title: "Sparse Bayesian Regression in High Dimensions: A Practical Exploration"
tags: ["blog", "sparse-bayesian-regression", "high-dimensions"]
date: 2025-04-27 23:37:00 +0800
cover: "./preview.png"
path: "posts/sparse-bayesian-regression"
excerpt: A practical exploration of Sparse Bayesian Regression techniques in high dimensions.
---

## Sparse Bayesian Regression in High Dimensions: A Practical Exploration

In this post, we explore how to apply `sklearn.BayesianRidge` to investigate sparsity and uncertainty in high-dimensional regression tasks.

---

### 1. Problem Statement

Suppose we are given a set of high-dimensional feature vectors `a` (from a design matrix `X`) and we aim to perform regression analysis where we not only estimate the coefficients but also quantify their uncertainties (posterior variances). Additionally, we are interested in observing the sparsity characteristics of the model coefficients under different levels of regularization.

The problem can be expressed mathematically as follows.

We observe a design matrix $X\in\mathbb{R}^{n\times d}$ and a response vector $\mathbf y\in\mathbb{R}^n$ generated by  

$
\mathbf y \;=\; X\,\mathbf w^{\!*} \;+\; \boldsymbol\varepsilon,
\quad
\boldsymbol\varepsilon \;\sim\; \mathcal N\!\bigl(\mathbf 0,\;\sigma^2 I_n\bigr),
$

where the ground-truth **sparsity vector** $\mathbf w^{\!*}\in\mathbb{R}^d$ has only $s\ll d$ non-zero entries.

Under the Bayesian-ridge assumption we place a Gaussian prior  

$
\mathbf w \;\sim\; \mathcal N\!\bigl(\mathbf 0,\;\lambda^{-1} I_d\bigr),
$

and—in the conjugate-Bayes setting—obtain the Gaussian posterior  

$
\mathbf w \mid X,\mathbf y
\;\sim\;
\mathcal N\!\bigl(\boldsymbol\mu,\;\Sigma\bigr),
\qquad
\boldsymbol\mu
=\bigl(X^{\!\top}X+\lambda I_d\bigr)^{-1}X^{\!\top}\mathbf y,
\qquad
\Sigma
=\alpha^{-1}\bigl(X^{\!\top}X+\lambda I_d\bigr)^{-1}.
$

Our goals are:

1. **Coefficient estimation and uncertainty**:  
   study the posterior mean $\boldsymbol\mu$ and covariance $\Sigma$ to quantify uncertainty.
2. **Sparsity analysis**:  
   examine how the estimated coefficients’ sparsity pattern changes under varying regularisation levels $(\alpha,\lambda)$, and compare it with the true sparsity vector $\mathbf w^{\!*}.$

---

### 2. Synthetic Dataset Generation

To rigorously analyze model behavior, we generate data as follows. Each element of the feature matrix $X$ is sampled from a standard normal distribution $X_{i,j} \sim \mathcal{N}(0,1)$. The true weight vector $w^*$ is constructed by randomly selecting $s$ nonzero elements and setting the rest to zero. The observation noise is sampled as $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$. Finally, the label vector $y$ is computed as $y = Xw^* + \varepsilon$.

---

### 3. Python Implementation

#### 3.1 Data Generation

> The true weight vector $w^*$ serves as the ground-truth sparsity vector, where only a small subset of coefficients are nonzero.

```python
import numpy as np
from sklearn.linear_model import BayesianRidge

def generate_synthetic_data(n, d, s, a=1.0, sigma=0.1, random_state=None):
    rng = np.random.default_rng(random_state)
    X = rng.standard_normal(size=(n, d))
    support = rng.choice(d, size=s, replace=False)
    w_true = np.zeros(d)
    w_true[support] = rng.uniform(-a, a, size=s)
    noise = rng.normal(0, sigma, size=n)
    y = X.dot(w_true) + noise
    return X, y, w_true
```

---

### 4. Analysis

#### 4.1 Posterior Uncertainty

The posterior standard deviation vector $\mathbf s$ (obtained as `w_std`) tells us how uncertain each weight is.  Averaged over the dataset we find
$\bar s_{\text{true‑nonzero}}\;\approx\;0.17,\qquad \bar s_{\text{true‑zero}}\;\approx\;0.09.$
True signals carry roughly twice the uncertainty of spurious ones, indicating that Bayesian Ridge does encode useful confidence information even though its prior is not sparsity‑inducing.

#### 4.2 Threshold Sensitivity
| Threshold $\tau$ | Non‑zeros | Precision | Recall |
|------------------|-----------|-----------|--------|
| $1\times10^{-2}$ | 782 | 0.023 | 0.900 |
| $3\times10^{-2}$ | 404 | 0.042 | 0.850 |
| $5\times10^{-2}$ | 177 | 0.085 | 0.750 |
| $7\times10^{-2}$ | 66  | 0.197 | 0.650 |

Raising $\tau$ removes many false positives (precision ↑) at the cost of missing some true coefficients (recall ↓).

#### 4.3 Support Recovery

With $\tau=10^{-2}$ the model recovers 18 / 20 true signals (high recall) but flags 764 false positives,
illustrating that a Gaussian prior alone is insufficient for sparse support identification.

#### 4.4 Predictive Performance & Generalisation

Training $R^2$ is 1.000 while test $R^2$ drops to 0.227, together with MSE = 0.006 and MAE = 0.036.
The gap confirms over‑fitting: the model fits $y$ well but learns a noisy weight vector.

#### 4.5 Effect of $n/d$ and Noise

Simulations (not shown) indicate that increasing $n$ (higher $n/d$) reduces false positives and tightens posterior intervals;
conversely, larger $\sigma$ inflates uncertainty, sometimes shrinking weights towards zero but also weakening detection of weak signals.

---

### 5. Conclusion

Bayesian Ridge meets **Goal 1** by providing closed‑form posterior means and variances: the latter successfully discriminate, to a degree, between true and inactive coefficients.  For **Goal 2**, however, the Gaussian prior does not enforce sparsity, leading to many false positives unless an aggressive threshold is applied.  Consequently the model enjoys excellent in‑sample fit yet limited out‑of‑sample performance.  Practitioners who need both accurate uncertainty quantification **and** sparse support should augment Bayesian Ridge with post‑hoc L1 projection, spike‑and‑slab, or ARD priors, and tune the threshold in concert with the data regime (sample size and noise level).