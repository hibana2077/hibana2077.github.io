{"componentChunkName":"component---node-modules-gatsby-theme-academic-src-templates-post-post-jsx-content-file-path-content-posts-sparse-bayesian-regression-in-high-dimensions-index-mdx","path":"/posts/sparse-bayesian-regression/","result":{"data":{"mdx":{"tableOfContents":{"items":[{"url":"#sparse-bayesian-regression-in-high-dimensions-a-practical-exploration","title":"Sparse Bayesian Regression in High Dimensions: A Practical Exploration","items":[{"url":"#1-problem-statement","title":"1. Problem Statement"},{"url":"#2-synthetic-dataset-generation","title":"2. Synthetic Dataset Generation"},{"url":"#3-python-implementation","title":"3. Python Implementation"},{"url":"#4-analysis","title":"4. Analysis","items":[{"url":"#41-threshold-sensitivity-analysis","title":"4.1 Threshold Sensitivity Analysis"},{"url":"#42-support-recovery-ability","title":"4.2 Support Recovery Ability"},{"url":"#43-predictive-performance","title":"4.3 Predictive Performance"},{"url":"#44-effects-of-sample-size-and-noise","title":"4.4 Effects of Sample Size and Noise"}]},{"url":"#5-conclusion","title":"5. Conclusion"}]}]},"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/5ea15f8d10300cbf21dd52a2d9900058/ffd11/preview.jpg","srcSet":"/static/5ea15f8d10300cbf21dd52a2d9900058/ffd11/preview.jpg 736w","sizes":"(min-width: 736px) 736px, 100vw"},"sources":[{"srcSet":"/static/5ea15f8d10300cbf21dd52a2d9900058/c5f3a/preview.webp 736w","type":"image/webp","sizes":"(min-width: 736px) 736px, 100vw"}]},"width":1000,"height":1000}}}},"fields":{"slug":{"html":"\n# Sparse Bayesian Regression in High Dimensions: A Practical Exploration\n\nIn this post, we explore how to apply `sklearn.BayesianRidge` to investigate sparsity and uncertainty in high-dimensional regression tasks.\n\n---\n\n## 1. Problem Statement\n\nSuppose we are given a set of high-dimensional feature vectors `a` (from a design matrix `X`) and we aim to perform regression analysis where we not only estimate the coefficients but also quantify their uncertainties (posterior variances). Additionally, we are interested in observing the sparsity characteristics of the model coefficients under different levels of regularization.\n\n---\n\n## 2. Synthetic Dataset Generation\n\nTo rigorously analyze model behavior, we generate data as follows. Each element of the feature matrix $X$ is sampled from a standard normal distribution $X_{i,j} \\sim \\mathcal{N}(0,1)$. The true weight vector $w^*$ is constructed by randomly selecting $s$ nonzero elements and setting the rest to zero. The observation noise is sampled as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Finally, the label vector $y$ is computed as $y = Xw^* + \\varepsilon$.\n\n---\n\n## 3. Python Implementation\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import BayesianRidge\n\ndef generate_synthetic_data(n, d, s, a=1.0, sigma=0.1, random_state=None):\n    rng = np.random.default_rng(random_state)\n    X = rng.standard_normal(size=(n, d))\n    support = rng.choice(d, size=s, replace=False)\n    w_true = np.zeros(d)\n    w_true[support] = rng.uniform(-a, a, size=s)\n    noise = rng.normal(0, sigma, size=n)\n    y = X.dot(w_true) + noise\n    return X, y, w_true\n\nn_samples, n_features, sparsity = 200, 1000, 20\nX, y, w_true = generate_synthetic_data(n=n_samples, d=n_features, s=sparsity, a=1.0, sigma=0.05, random_state=42)\n\nmodel = BayesianRidge(compute_score=True)\nmodel.fit(X, y)\nw_mean = model.coef_\nw_std = np.sqrt(model.sigma_)\n```\n\n---\n\n## 4. Analysis\n\n### 4.1 Threshold Sensitivity Analysis\n\nThe relationship between different thresholds $\\tau$, the number of estimated nonzero coefficients, precision, and recall is summarized below:\n\n| Threshold $\\tau$ | Estimated Nonzero Count | Precision | Recall |\n|:---------------:|:------------------------:|:---------:|:------:|\n| $1.0\\times10^{-2}$ | 782 | 0.023 | 0.900 |\n| $3.0\\times10^{-2}$ | 404 | 0.042 | 0.850 |\n| $5.0\\times10^{-2}$ | 177 | 0.085 | 0.750 |\n| $7.0\\times10^{-2}$ | 66  | 0.197 | 0.650 |\n\nAs the threshold increases, the precision improves while the recall declines. A higher precision but lower recall indicates that although fewer false positives occur, more true features are missed.\n\n### 4.2 Support Recovery Ability\n\nIn the generated data, there are 20 true nonzero coefficients. When setting the threshold at $1\\times10^{-2}$, the model estimates 782 nonzero coefficients. This result reflects a typical over-selection phenomenon: high recall but extremely low precision.\n\n### 4.3 Predictive Performance\n\nThe model achieves a training $R^2$ score of 1.000 and a testing $R^2$ score of 0.227. The mean squared error (MSE) is 0.006, the mean absolute error (MAE) is 0.036, and the Pearson correlation between estimated and true coefficients is 0.472. Although the coefficient estimation error is noticeable, the model performs nearly perfectly on the training data. However, the significantly lower testing performance indicates a degree of overfitting.\n\n### 4.4 Effects of Sample Size and Noise\n\nWhen the sample-to-feature ratio $n/d$ increases (i.e., more samples relative to features), the number of false positives is expected to decrease. Additionally, increasing the observation noise enlarges the posterior variances, encouraging coefficients to shrink towards zero, thus enhancing sparsity. However, higher noise can also increase the risk of missing true nonzero features.\n\n---\n\n## 5. Conclusion\n\nBayesian Ridge regression can effectively recover most important features, achieving high recall. However, it struggles to enforce sparsity naturally, leading to low precision. In high-dimensional sparse problems, although training fit is excellent, the generalization performance is limited, revealing a risk of overfitting. To improve sparsity or generalization ability, one should consider combining feature selection methods such as L1 projection or using models that enforce stronger sparsity priors like ARD (Automatic Relevance Determination). The choice of threshold and the conditions of the data, including sample size and noise level, critically influence the outcomes and should be carefully tuned according to specific application needs.\n\n","htmlEncrypted":"","nonce":"","timeToRead":null,"title":"Sparse Bayesian Regression in High Dimensions: A Practical Exploration","date":"1743465600000","tags":["blog","sparse-bayesian-regression","high-dimensions"],"path":"posts/sparse-bayesian-regression","excerpt":"A practical exploration of Sparse Bayesian Regression techniques in high dimensions.","links":[],"commit":0,"type":"posts"}},"internal":{"contentFilePath":"/home/runner/work/hibana2077.github.io/hibana2077.github.io/content/posts/Sparse Bayesian Regression in High Dimensions/index.mdx"}}},"pageContext":{"contentFilePath":"/home/runner/work/hibana2077.github.io/hibana2077.github.io/content/posts/Sparse Bayesian Regression in High Dimensions/index.mdx","postPath":"posts/sparse-bayesian-regression","translations":[{"hreflang":"en","path":"/posts/sparse-bayesian-regression"}],"frontmatter":{"title":"Sparse Bayesian Regression in High Dimensions: A Practical Exploration","tags":["blog","sparse-bayesian-regression","high-dimensions"],"date":"2025-04-01T00:00:00.000Z","cover":"./preview.png","path":"posts/sparse-bayesian-regression","excerpt":"A practical exploration of Sparse Bayesian Regression techniques in high dimensions."}}},"staticQueryHashes":["1552981879","2158328490","3013679938"],"slicesMap":{}}