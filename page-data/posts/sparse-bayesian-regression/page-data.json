{"componentChunkName":"component---node-modules-gatsby-theme-academic-src-templates-post-post-jsx-content-file-path-content-posts-sparse-bayesian-regression-in-high-dimensions-index-mdx","path":"/posts/sparse-bayesian-regression/","result":{"data":{"mdx":{"tableOfContents":{"items":[{"url":"#sparse-bayesian-regression-in-high-dimensions-a-practical-exploration","title":"Sparse Bayesian Regression in High Dimensions: A Practical Exploration","items":[{"url":"#1-problem-statement","title":"1. Problem Statement"},{"url":"#2-synthetic-dataset-generation","title":"2. Synthetic Dataset Generation"},{"url":"#3-python-implementation","title":"3. Python Implementation","items":[{"url":"#31-data-generation","title":"3.1 Data Generation"}]},{"url":"#4-analysis","title":"4. Analysis","items":[{"url":"#41-posterior-uncertainty","title":"4.1 Posterior Uncertainty"},{"url":"#42-threshold-sensitivity","title":"4.2 Threshold Sensitivity"},{"url":"#43-support-recovery","title":"4.3 Support Recovery"},{"url":"#44-predictive-performance--generalisation","title":"4.4 Predictive Performance & Generalisation"},{"url":"#45-effect-of-nd-and-noise","title":"4.5 Effect of  and Noise"}]},{"url":"#5-conclusion","title":"5. Conclusion"}]}]},"frontmatter":{"cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/5ea15f8d10300cbf21dd52a2d9900058/ffd11/preview.jpg","srcSet":"/static/5ea15f8d10300cbf21dd52a2d9900058/ffd11/preview.jpg 736w","sizes":"(min-width: 736px) 736px, 100vw"},"sources":[{"srcSet":"/static/5ea15f8d10300cbf21dd52a2d9900058/c5f3a/preview.webp 736w","type":"image/webp","sizes":"(min-width: 736px) 736px, 100vw"}]},"width":1000,"height":1000}}}},"fields":{"slug":{"html":"\n## Sparse Bayesian Regression in High Dimensions: A Practical Exploration\n\nIn this post, we explore how to apply `sklearn.BayesianRidge` to investigate sparsity and uncertainty in high-dimensional regression tasks.\n\n---\n\n### 1. Problem Statement\n\nSuppose we are given a set of high-dimensional feature vectors `a` (from a design matrix `X`) and we aim to perform regression analysis where we not only estimate the coefficients but also quantify their uncertainties (posterior variances). Additionally, we are interested in observing the sparsity characteristics of the model coefficients under different levels of regularization.\n\nThe problem can be expressed mathematically as follows.\n\nWe observe a design matrix $X\\in\\mathbb{R}^{n\\times d}$ and a response vector $\\mathbf y\\in\\mathbb{R}^n$ generated by  \n\n$\n\\mathbf y \\;=\\; X\\,\\mathbf w^{\\!*} \\;+\\; \\boldsymbol\\varepsilon,\n\\quad\n\\boldsymbol\\varepsilon \\;\\sim\\; \\mathcal N\\!\\bigl(\\mathbf 0,\\;\\sigma^2 I_n\\bigr),\n$\n\nwhere the ground-truth **sparsity vector** $\\mathbf w^{\\!*}\\in\\mathbb{R}^d$ has only $s\\ll d$ non-zero entries.\n\nUnder the Bayesian-ridge assumption we place a Gaussian prior  \n\n$\n\\mathbf w \\;\\sim\\; \\mathcal N\\!\\bigl(\\mathbf 0,\\;\\lambda^{-1} I_d\\bigr),\n$\n\nand—in the conjugate-Bayes setting—obtain the Gaussian posterior  \n\n$\n\\mathbf w \\mid X,\\mathbf y\n\\;\\sim\\;\n\\mathcal N\\!\\bigl(\\boldsymbol\\mu,\\;\\Sigma\\bigr),\n\\qquad\n\\boldsymbol\\mu\n=\\bigl(X^{\\!\\top}X+\\lambda I_d\\bigr)^{-1}X^{\\!\\top}\\mathbf y,\n\\qquad\n\\Sigma\n=\\alpha^{-1}\\bigl(X^{\\!\\top}X+\\lambda I_d\\bigr)^{-1}.\n$\n\nOur goals are:\n\n1. **Coefficient estimation and uncertainty**:  \n   study the posterior mean $\\boldsymbol\\mu$ and covariance $\\Sigma$ to quantify uncertainty.\n2. **Sparsity analysis**:  \n   examine how the estimated coefficients’ sparsity pattern changes under varying regularisation levels $(\\alpha,\\lambda)$, and compare it with the true sparsity vector $\\mathbf w^{\\!*}.$\n\n---\n\n### 2. Synthetic Dataset Generation\n\nTo rigorously analyze model behavior, we generate data as follows. Each element of the feature matrix $X$ is sampled from a standard normal distribution $X_{i,j} \\sim \\mathcal{N}(0,1)$. The true weight vector $w^*$ is constructed by randomly selecting $s$ nonzero elements and setting the rest to zero. The observation noise is sampled as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Finally, the label vector $y$ is computed as $y = Xw^* + \\varepsilon$.\n\n---\n\n### 3. Python Implementation\n\n#### 3.1 Data Generation\n\n> The true weight vector $w^*$ serves as the ground-truth sparsity vector, where only a small subset of coefficients are nonzero.\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import BayesianRidge\n\ndef generate_synthetic_data(n, d, s, a=1.0, sigma=0.1, random_state=None):\n    rng = np.random.default_rng(random_state)\n    X = rng.standard_normal(size=(n, d))\n    support = rng.choice(d, size=s, replace=False)\n    w_true = np.zeros(d)\n    w_true[support] = rng.uniform(-a, a, size=s)\n    noise = rng.normal(0, sigma, size=n)\n    y = X.dot(w_true) + noise\n    return X, y, w_true\n```\n\n---\n\n### 4. Analysis\n\n#### 4.1 Posterior Uncertainty\n\nThe posterior standard deviation vector $\\mathbf s$ (obtained as `w_std`) tells us how uncertain each weight is.  Averaged over the dataset we find\n$\\bar s_{\\text{true‑nonzero}}\\;\\approx\\;0.17,\\qquad \\bar s_{\\text{true‑zero}}\\;\\approx\\;0.09.$\nTrue signals carry roughly twice the uncertainty of spurious ones, indicating that Bayesian Ridge does encode useful confidence information even though its prior is not sparsity‑inducing.\n\n#### 4.2 Threshold Sensitivity\n| Threshold $\\tau$ | Non‑zeros | Precision | Recall |\n|------------------|-----------|-----------|--------|\n| $1\\times10^{-2}$ | 782 | 0.023 | 0.900 |\n| $3\\times10^{-2}$ | 404 | 0.042 | 0.850 |\n| $5\\times10^{-2}$ | 177 | 0.085 | 0.750 |\n| $7\\times10^{-2}$ | 66  | 0.197 | 0.650 |\n\nRaising $\\tau$ removes many false positives (precision ↑) at the cost of missing some true coefficients (recall ↓).\n\n#### 4.3 Support Recovery\n\nWith $\\tau=10^{-2}$ the model recovers 18 / 20 true signals (high recall) but flags 764 false positives,\nillustrating that a Gaussian prior alone is insufficient for sparse support identification.\n\n#### 4.4 Predictive Performance & Generalisation\n\nTraining $R^2$ is 1.000 while test $R^2$ drops to 0.227, together with MSE = 0.006 and MAE = 0.036.\nThe gap confirms over‑fitting: the model fits $y$ well but learns a noisy weight vector.\n\n#### 4.5 Effect of $n/d$ and Noise\n\nSimulations (not shown) indicate that increasing $n$ (higher $n/d$) reduces false positives and tightens posterior intervals;\nconversely, larger $\\sigma$ inflates uncertainty, sometimes shrinking weights towards zero but also weakening detection of weak signals.\n\n---\n\n### 5. Conclusion\n\nBayesian Ridge meets **Goal 1** by providing closed‑form posterior means and variances: the latter successfully discriminate, to a degree, between true and inactive coefficients.  For **Goal 2**, however, the Gaussian prior does not enforce sparsity, leading to many false positives unless an aggressive threshold is applied.  Consequently the model enjoys excellent in‑sample fit yet limited out‑of‑sample performance.  Practitioners who need both accurate uncertainty quantification **and** sparse support should augment Bayesian Ridge with post‑hoc L1 projection, spike‑and‑slab, or ARD priors, and tune the threshold in concert with the data regime (sample size and noise level).","htmlEncrypted":"","nonce":"","timeToRead":null,"title":"Sparse Bayesian Regression in High Dimensions: A Practical Exploration","date":"2025-04-27 23:37:00 +0800","tags":["blog","sparse-bayesian-regression","high-dimensions"],"path":"posts/sparse-bayesian-regression","excerpt":"A practical exploration of Sparse Bayesian Regression techniques in high dimensions.","links":[],"commit":0,"type":"posts"}},"internal":{"contentFilePath":"/home/runner/work/hibana2077.github.io/hibana2077.github.io/content/posts/Sparse Bayesian Regression in High Dimensions/index.mdx"}}},"pageContext":{"contentFilePath":"/home/runner/work/hibana2077.github.io/hibana2077.github.io/content/posts/Sparse Bayesian Regression in High Dimensions/index.mdx","postPath":"posts/sparse-bayesian-regression","translations":[{"hreflang":"en","path":"/posts/sparse-bayesian-regression"}],"frontmatter":{"title":"Sparse Bayesian Regression in High Dimensions: A Practical Exploration","tags":["blog","sparse-bayesian-regression","high-dimensions"],"date":"2025-04-27 23:37:00 +0800","cover":"./preview.png","path":"posts/sparse-bayesian-regression","excerpt":"A practical exploration of Sparse Bayesian Regression techniques in high dimensions."}}},"staticQueryHashes":["1552981879","2158328490","3013679938"],"slicesMap":{}}